---
title: "A Review of Statistics and Probability Fundamentals"
author: "Timothy Pace"
date: "2/11/2017"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## In 2013, the Pew Research Foundation reported that “45% of U.S. adults report that they live with one or more chronic conditions”. However, this value was based on a sample, so it may not be a perfect estimate for the population parameter of interest on its own. The study reported a standard error of about 1.2%, and a normal model may reasonably be used in this setting. Create a 95% confidence interval for the proportion of U.S. adults who live with one or more chronic conditions. Also interpret the confidence interval in the context of the study.

```{r}
SE <- 0.012
p_hat <- 0.45
Z <- 1.96

upper <- p_hat + Z * SE
lower <- p_hat - Z * SE

upper
lower
```
(95% CI: 0.42648, 0.47352)

We are 95% confident that the true proportion of U.S. adults who live with one or more chronic conditions is within (0.42648, 0.47352).

In other words, if we take many samples, 95% of the time we repeat this process, our confidence intervals will contain the true proportion of U.S. adults who live with one or more chronic conditions. 

## The nutrition label on a bag of potato chips says that a one ounce (28 gram) serving of potato chips has 130 calories and contains ten grams of fat, with three grams of saturated fat. A random sample of 35 bags yielded a sample mean of 136 calories with a standard deviation of 17 calories. Write down the null and alternative hypotheses for a two-sided test of whether the nutrition label is lying.


Null Hypothesis: 


The mean calories of a one ounce bag of potato chips is equal to 130 calories and the nutrition label is not lying.


$\mu$ = 130

Alternate Hypothesis: 


The mean calories of a one ounce bag of potato chips is not equal to 130 calories and the nutrition label is lying.


$\mu \neq 130$


Calculate the test statistic and find the p value.
$$ \frac{\bar{X} - \mu}{\sigma_{\bar{X}}}$$

```{r}
n <- 35
x_bar <- 136
mu <- 130
std_dev <- 17

test_s <- (x_bar - mu) / (std_dev/sqrt(35))
test_s

upper_tail <- pnorm(test_s, lower.tail = F)
lower_tail <- pnorm(-test_s, lower.tail = T)

p_value <- lower_tail + lower_tail
p_value
```
The test statistic is: 2.088028.  The p-value is: 0.03679529.

## If you were the potato chip company would you rather have your alpha = 0.05 or 0.025 in this case? Why?

If you were the potato chip company in this case, you would rather have your alpha equal to 0.025, because if the alpha is equal to 0.05, we reject the null hypothesis ($p$ < 0.05), indicating that the nutrition label is lying. However, if we set an alpha equal to 0.025, we fail to reject the null hypothesis that the nutrition label isn't lying ($p$ > 0.025), thereby benefiting the potato chip company.

The potato chip company would also want an alpha equal to 0.025 because, for a two-tailed test, you divide the alpha by two for each tail. Therefore, if an alpha of 0.025 is used, a p-value of less than 0.0125 is required in each tail, which would make it harder to obtain statistical significance. This is a good thing for the potato chip company, as it would be beneficial to increase the likelihood of committing a type II error.

## Regression was originally used by Francis Galton to study the relationship between parents and children. He wondered if he could predict a man’s height based on the height of his father? This is the question we will explore in this problem. You can obtain data similar to that used by Galton as follows:

```{r}
#install.packages("UsingR")
library(UsingR)
height <- get("father.son")
```


### Perform an exploratory analysis of the father and son heights. What does the relationship look like? Would a linear model be appropriate here?
```{r}
dim(height)
str(height)
summary(height)
cor(height)
plot(height$sheight ~ height$fheight, xlab = "Father Heights", ylab = "Son Heights", 
     main = "Father vs. Son Heights")
```


The relationship is positive and increasing. In other words, there is a positive association between father and son heights, with son heights increasing in association with father heights.

A linear model would be appropriate here, as the correlation is moderately positive (r = 0.50), and data appears to be linearly shaped when plotted.

### Fit a simple linear regression model to predict son's height as a function of father's height. Write down the model, ysheight = B0 + B1 x fheight filling in estimated coefficient values and interpret the coefficient estimates.
```{r}
sheight_model <- lm(sheight ~ fheight, data = height)
summary(sheight_model)
```
ysheight = $\beta_0 + \beta_1$ x fheight


ysheight = 33.88660 + 0.51409 x fheight


B1: A 1 inch increase in father height is associated with an expected increase of 0.51409 inches in son height. 

B0: A father of height 0 would be expected to have a son with a height of 33.88660 inches.


### Find the 95% confidence intervals for the estimates. You may find the confint() command useful.
```{r}
confint(sheight_model, level = 0.95)
```
0.51409 (95% CI: 0.4610188, 0.5671673).

### Produce a visualization of the data and the least squares regression line.
```{r}
plot(height$sheight ~ height$fheight, xlab = "Father Heights", ylab = "Son Heights", 
     main = "Father vs. Son Heights")
abline(sheight_model, col = "red")
```


### Produce a visualization of the residuals versus the fitted values. Discuss what you see. Do you have any concerns about the linear model?

```{r}
names(sheight_model)
qqplot(sheight_model[["fitted.values"]], sheight_model[["residuals"]], 
       xlab = "Fitted Values", 
       ylab = "Residuals", 
       main = "Residuals vs. Fitted Values Q-Q Plot")

plot(sheight_model[["residuals"]] ~ sheight_model[["fitted.values"]],
      xlab = "Fitted Values", 
       ylab = "Residuals", 
       main = "Residuals vs. Fitted Values")
abline(h = 0)

plot(sheight_model)
```


On the Q-Q plot, the distribution of the residuals is generally normal and falls on a straight line. However, the distribution of the residuals is not necessarily normal in the quantiles below -2 and above 2, and the tails of the residuals distribution may be slightly non-normal. With that said, there is little data in the tails of the distribution, so it is difficult to tell. Therefore, the linear model mostly meets the assumption of normal residuals.

In the Residuals vs Fitted values plot, the variability of the points around the least squares line remains roughly constant. Because the line is more or less flat, the pattern is homoscedastic, and the linear model therefore meets the assumption of constant variability.

We therefore have no major concerns about the linear model.

### Using the model you fit in part (b) predict the height was 5 males whose father are 50, 55, 70, 75, and 90 inches respectively. 

```{r}
predict(sheight_model, newdata = data.frame("fheight" = c(50, 55, 70, 75, 90)),
        interval = "prediction")
```
The predicted heights are (father height: expected son height): 50: 59.59126, 55: 62.16172 , 70: 69.87312 , 75: 72.44358, 90: 80.15498.

### What do the estimates of the slope and height mean? Are the results statistically significant? Are they practically significant?

A 1 inch increase in father height is associated with an expected increase of 0.51409 inches in son height. Moreover, a father of height 0 would be expected to have a son with a height of 33.88660 inches.

The results for the estimates of the slope of height are statistically significant, with a small p-value of 2.2e-16 ($p$ < 0.001).

It is practically significant that a father's height being higher would mean that a son's height is higher as well (e.g., due to genetics). However, there are many factors and confounding variables other than father's height that may be contributing to the son's height. For example, other factors that may be influencing son's height might include the mother's height, as well as enviromental variables such as proper nutrition and disease.  Moreover, the adjusted r-squared is 0.2506, meaning that only 0.25 of the variability in the son's height is explained by the father's height. 


### An investigator is interested in understanding the relationship, if any, between the analytical skills of young gifted children and the father's IQ, the mother's IQ, and hours of educational TV. The data are here:

```{r}
#install.packages("openintro")
library(openintro)
data(gifted)
```


### Run two regressions: one with the child's analytical skills test score (“score”) and the father's IQ (“fatheriq”) and the child's score and the mother's IQ score (“motheriq”).
```{r}
plot(gifted$score ~ gifted$fatheriq, xlab = "Father\'s IQ", 
     ylab = "Child\'s Analytical Skills Test Score", 
     main = "Child\'s Analytical Skills Test Score vs Father\'s IQ")

father_model <- lm(score ~ fatheriq, data = gifted)
abline(father_model, col = "red")

plot(gifted$score ~ gifted$motheriq, xlab = "Mother\'s IQ", 
     ylab = "Child\'s Analytical Skills Test Score", 
     main = "Child\'s Analytical Skills Test Score vs Mother\'s IQ")

mother_model <- lm(score ~ motheriq, data = gifted)
abline(mother_model, col = "red")
```


### What are the estimates of the slopes for father and mother's IQ score with their 95% confidence intervals? (Note, estimates and confidence intervals are usually reported: Estimate (95% CI: CIlower, CIupper)


```{r}
summary(father_model)
confint(father_model, level = 0.95)

summary(mother_model)
confint(mother_model, level = 0.95)
```



Father's IQ score slope estimate: 0.2501 (95% CI: -0.2051068, 0.7053687)

Mother's IQ score slope estimate: 0.4066 (95% CI: 0.2029815, 0.6102077)


### How are these interpreted?


A 1 point increase in the father's IQ score is associated with an expected increase of 0.2501 points on the child's analytical skills test score. With 95% confidence, the true value of the slope is within -0.2051068 and 0.7053687. If we repeated this experiment many times, the true slope of the linear model would be contained within our confidence intervals 95% of the time.



A 1 point increase in the mother's IQ score is associated with an expected increase of 0.4066 points in the child's analytical skills test score. With 95% confidence, the true value of the slope of the linear model is within -0.2051068 and 0.7053687. If we repeated this experiment many times, the true slope of the linear model would be contained within our confidence intervals 95% of the time.

### What conclusions can you draw about the association between the child's score and the mother and father's IQ?

The adjusted R-squared for the model of mother's IQ (0.3065) is greater than the adjusted R-squared of the model of father's IQ  (0.007003) in association with childrens' analytical skills. Because mother's IQ has a higher adjusted R-squared, it explains more of the variation of the data and is more strongly associated with childrens' analytical skills test scores. Therefore, compared with father's IQ, the mother's IQ explains the childrens' score data best. 


## For each of the following situations, state whether the parameter of interest is a mean or a proportion. It may be helpful to examine whether individual responses are numerical or categorical.

In a survey, one hundred college students are asked how many hours per week they spend on the Internet.


Mean

In a survey, one hundred college students are asked: “What percentage of the time you spend on the Internet is part of your course work?”


Mean

In a survey, one hundred college students are asked whether or not they cited information from Wikipedia in their papers.


Proportion

In a survey, one hundred college students are asked what percentage of their total weekly spending is on alcoholic beverages.


Mean

In a sample of one hundred recent college graduates, it is found that 85 percent expect to get a job within one year of their graduation date.


Proportion